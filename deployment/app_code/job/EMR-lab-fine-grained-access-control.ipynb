{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81deb244",
   "metadata": {},
   "source": [
    "# 0. Load sagemaker_studio_analytics_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker-studio-analytics-extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5bf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bf409",
   "metadata": {},
   "source": [
    "# 1. Act as data engineer \n",
    "\n",
    "\n",
    ">**In this section, we will create Spark application with *data engineer* EMR runtime role, which is designed as a Lake Formation database and table creator. Then, we will performce Create, Read, Insert, Drop, a.k.a CRUD, actions on the databases and tables that are governed by Lake Formation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df115874",
   "metadata": {},
   "source": [
    "## 1.1. Connect to EMR cluster via Livy as the ENGINEER_ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a8651c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster ID:   j-1NEOGU3MXB8YT\n",
      "IAM ARN:      arn:aws:iam::010261715632:role/lf-data-access-engineer\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "source ~/.bash_profile\n",
    "EMR_CLUSTER_ID=$(aws emr list-clusters --active  --query 'Clusters[?contains(Name,`emr-roadshow-runtime-role-lf`)].Id' --output text)\n",
    "echo \"cluster ID:   $EMR_CLUSTER_ID\"\n",
    "echo \"IAM ARN:      $ENGINEER_ROLE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e748c00",
   "metadata": {},
   "source": [
    ">**In the following command, replace `<CLUSTER_ID>` with 'cluster ID', and `<IAM_ARN>` with 'IAM ARN' using the output from the above cell.**\n",
    "\n",
    "Example:\n",
    "```\n",
    "%sm_analytics emr connect \\\n",
    "--cluster-id j-1NEOGU3MXB8YT \\\n",
    "--auth-type Basic_Access \\\n",
    "--emr-execution-role-arn arn:aws:iam::012345678:role/lf-data-access-engineer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sm_analytics emr connect \\\n",
    "--cluster-id <CLUSTER_ID> \\\n",
    "--auth-type Basic_Access \\\n",
    "--emr-execution-role-arn <IAM_ARN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457eb53",
   "metadata": {},
   "source": [
    "## 1.2. Config parameters for S3 data lake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e04f48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATALAKE_BUCKET:    lf-datalake-010261715632-us-east-1\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "source ~/.bash_profile\n",
    "echo \"DATALAKE_BUCKET:    $DATALAKE_BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246b73f",
   "metadata": {},
   "source": [
    "**Replace `DATALAKE_BUCKET` with the output from the above cell.**\n",
    "\n",
    "Example:\n",
    "```\n",
    "DATALAKE_BUCKET=\"lf-datalake-012345657-us-east-1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALAKE_BUCKET=\"<DATALAKE_BUCKET>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06417216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212dafde70454fd695838063a4f1abcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import concat, col, lit, to_timestamp, dense_rank, desc, count, rand, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "rawS3TablePath = f\"s3://{DATALAKE_BUCKET}/raw/ticket_purchase_hist/\"\n",
    "outputS3Path = f\"s3://{DATALAKE_BUCKET}/output/\"\n",
    "\n",
    "targetDBName = 'sample'\n",
    "targetTableName = 'ticket_purchase_hist'\n",
    "targetPath = os.path.join(outputS3Path, targetDBName, targetTableName)\n",
    "\n",
    "targetDBName2 = 'sample2'\n",
    "targetTableName2 = 'ticket_purchase_hist2'\n",
    "targetPath2 = os.path.join(outputS3Path, targetDBName2, targetTableName2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f3a02",
   "metadata": {},
   "source": [
    "## 1.3. Create database and table governed by Lake Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea59fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a79c920a7a64eb0ae1ed00e290dedf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {targetDBName}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ca18112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acafa175f1bb42f59ea4b1d801ac3f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {targetDBName}.{targetTableName} (\n",
    "        sporting_event_ticket_id string,\n",
    "        purchased_by_id string,\n",
    "        transaction_date_time string,\n",
    "        transferred_from_id string,\n",
    "        purchase_price string\n",
    "    )\n",
    "    USING PARQUET\n",
    "    OPTIONS(\n",
    "        'path' '{targetPath}'\n",
    "    )\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90c11343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c421fa582a84f07a2f932d315cdf77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {targetDBName2}\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {targetDBName2}.{targetTableName2} LIKE {targetDBName}.{targetTableName}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eaa159",
   "metadata": {},
   "source": [
    "## 1.4 Load data to created table governed by Lake Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05220541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4080238eb5d4672b104f653f74e190d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sporting_event_ticket_id: string (nullable = true)\n",
      " |-- purchased_by_id: string (nullable = true)\n",
      " |-- transaction_date_time: string (nullable = true)\n",
      " |-- transferred_from_id: string (nullable = true)\n",
      " |-- purchase_price: string (nullable = true)\n",
      "\n",
      "+------------------------+-----------------------+---------------------+-------------------+--------------+\n",
      "|sporting_event_ticket_id|purchased_by_id        |transaction_date_time|transferred_from_id|purchase_price|\n",
      "+------------------------+-----------------------+---------------------+-------------------+--------------+\n",
      "|+1.2295361000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |47.30         |\n",
      "|+1.2294881000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |37.84         |\n",
      "|+1.2294891000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |37.84         |\n",
      "|+1.2295351000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |47.30         |\n",
      "|+1.2294871000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |37.84         |\n",
      "|+1.2294861000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |37.84         |\n",
      "|+1.2295321000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |47.30         |\n",
      "|+1.2295341000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |47.30         |\n",
      "|+1.2295331000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |47.30         |\n",
      "|+1.2294851000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |37.84         |\n",
      "+------------------------+-----------------------+---------------------+-------------------+--------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "inputDf = spark.read.option(\"header\", True).csv(rawS3TablePath)\n",
    "inputDf.printSchema()\n",
    "inputDf.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "096e5cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac0242abe10428882a1baa6bf4f8d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputDf.write.insertInto(f\"{targetDBName}.{targetTableName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc15143",
   "metadata": {},
   "source": [
    "## 1.5. Read data from created table governed by Lake Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c4f9f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b9cf62d7114b1e8c10c66d396ba968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------+---------------------+-------------------+--------------+\n",
      "|sporting_event_ticket_id|purchased_by_id        |transaction_date_time|transferred_from_id|purchase_price|\n",
      "+------------------------+-----------------------+---------------------+-------------------+--------------+\n",
      "|+1.2295361000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |47.30         |\n",
      "|+1.2294881000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |37.84         |\n",
      "|+1.2294891000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |37.84         |\n",
      "|+1.2295351000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |47.30         |\n",
      "|+1.2294871000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |37.84         |\n",
      "|+1.2294861000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |37.84         |\n",
      "|+1.2295321000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |47.30         |\n",
      "|+1.2295341000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |47.30         |\n",
      "|+1.2295331000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |47.30         |\n",
      "|+1.2294851000000000e+07 |+4.5526330000000000e+06|2023-01-23 06:36:25  |null               |37.84         |\n",
      "+------------------------+-----------------------+---------------------+-------------------+--------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {targetDBName}.{targetTableName}\n",
    "\"\"\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974d19e",
   "metadata": {},
   "source": [
    "## 1.6. Insert data to created table governed by Lake Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce2eef3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b17562f1bba4a06bdee69c2fc4aa443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_df = spark.read.option(\"header\", True).csv(rawS3TablePath).sample(fraction=0.1).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4c21026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7456f45295ae46f8a21904fadede8786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "rand_df.createOrReplaceTempView(\"tmp_table\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {targetDBName}.{targetTableName}\n",
    "    SELECT *\n",
    "    FROM tmp_table\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c932d",
   "metadata": {},
   "source": [
    "## 1.7. Drop database and table governed by Lake Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff7bb6a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aab93cebb38411d88fb1cc04108dd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    DROP TABLE {targetDBName2}.{targetTableName2}\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    DROP DATABASE {targetDBName2}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3982505",
   "metadata": {},
   "source": [
    "# 2. Act as data analyst \n",
    "\n",
    "\n",
    ">**In this section, we will create Spark application with *data analyst* EMR runtime role, which is designed as a consumer for the table that has fine-grained access control by Lake Formation.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c3229",
   "metadata": {},
   "source": [
    "## 2.1. Connect to EMR cluster via Livy as ANALYST_ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64129bdf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j-1NEOGU3MXB8YT\n",
      "arn:aws:iam::010261715632:role/lf-data-access-analyst\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "source ~/.bash_profile\n",
    "EMR_CLUSTER_ID=$(aws emr list-clusters --active  --query 'Clusters[?contains(Name,`emr-roadshow-runtime-role-lf`)].Id' --output text)\n",
    "echo $EMR_CLUSTER_ID\n",
    "echo $ANALYST_ROLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sm_analytics emr connect \\\n",
    "--cluster-id <CLUSTER_ID> \\\n",
    "--auth-type Basic_Access \\\n",
    "--emr-execution-role-arn <ANALYST_ROLE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7066fd3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATALAKE_BUCKET:    lf-datalake-010261715632-us-east-1\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "source ~/.bash_profile\n",
    "echo \"DATALAKE_BUCKET:    $DATALAKE_BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841519df",
   "metadata": {},
   "source": [
    "**Replace `DATALAKE_BUCKET` with the output from the above cell.**\n",
    "\n",
    "Example:\n",
    "```\n",
    "DATALAKE_BUCKET=\"lf-datalake-012345657-us-east-1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c16847",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALAKE_BUCKET=\"<DATALAKE_BUCKET>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e05ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd95cecd63b437399a6c1f1341bdd00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import concat, col, lit, to_timestamp, dense_rank, desc, count, rand, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "outputS3Path = f\"s3://{DATALAKE_BUCKET}/output/\"\n",
    "\n",
    "rawS3TablePath = f\"s3://{DATALAKE_BUCKET}/raw/ticket_purchase_hist/\"\n",
    "\n",
    "targetDBName = 'sample'\n",
    "targetTableName = 'ticket_purchase_hist'\n",
    "targetPath = os.path.join(outputS3Path, targetDBName, targetTableName)\n",
    "\n",
    "targetDBName2 = 'sample2'\n",
    "targetTableName2 = 'ticket_purchase_hist2'\n",
    "targetPath2 = os.path.join(outputS3Path, targetDBName2, targetTableName2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69eda6",
   "metadata": {},
   "source": [
    "## 2.2 Test the column-level permission\n",
    "\n",
    "> Please grant ANALYST_ROLE permission via Lake Formation following the construction in workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76af52e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6082fbbd8fb465b8d79ff9a5805d0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------+\n",
      "|sporting_event_ticket_id|purchase_price|\n",
      "+------------------------+--------------+\n",
      "|+1.2295361000000000e+07 |47.30         |\n",
      "|+1.2294881000000000e+07 |37.84         |\n",
      "|+1.2294891000000000e+07 |37.84         |\n",
      "|+1.2295351000000000e+07 |47.30         |\n",
      "|+1.2294871000000000e+07 |37.84         |\n",
      "|+1.2294861000000000e+07 |37.84         |\n",
      "|+1.2295321000000000e+07 |47.30         |\n",
      "|+1.2295341000000000e+07 |47.30         |\n",
      "|+1.2295331000000000e+07 |47.30         |\n",
      "|+1.2294851000000000e+07 |37.84         |\n",
      "+------------------------+--------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {targetDBName}.{targetTableName}\n",
    "\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b2b8c",
   "metadata": {},
   "source": [
    "**The above cell's output expects to only have columns granted from Lake Formation for analyst role**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00a7c8",
   "metadata": {},
   "source": [
    "## 2.3 Test insert permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8dcd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b76624dd114b24859a706c3636c45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_df = spark.read.option(\"header\", True).csv(rawS3TablePath).sample(fraction=0.1).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee04dfed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc50733e66e4491b0810d8b2c12005a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "com.amazonaws.emr.recordserver.remote.AuthorizationException: Permission Denied: User WWGEYX2KNJRORBC2XW2IVZ422YM62EZK does not have INSERT permission on sample/ticket_purchase_hist\n",
      "\tat com.amazonaws.emr.recordserver.authz.AuthorizerImpl.authorize(Authorizer.scala:392)\n",
      "\tat com.amazonaws.emr.recordserver.handler.AuthorizationHandler.$anonfun$channelRead0$1(AuthorizationHandler.scala:33)\n",
      "\tat com.amazonaws.emr.recordserver.metrics.Metrics$.withMetering(Metrics.scala:49)\n",
      "\tat com.amazonaws.emr.recordserver.handler.AuthorizationHandler.channelRead0(AuthorizationHandler.scala:33)\n",
      "\tat com.amazonaws.emr.recordserver.handler.AuthorizationHandler.channelRead0(AuthorizationHandler.scala:21)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat com.amazonaws.emr.recordserver.handler.ValidationHandler.channelRead0(ValidationHandler.scala:28)\n",
      "\tat com.amazonaws.emr.recordserver.handler.ValidationHandler.channelRead0(ValidationHandler.scala:17)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat com.amazonaws.emr.recordserver.handler.AuthenticationHandler.channelRead0(AuthenticationHandler.scala:66)\n",
      "\tat com.amazonaws.emr.recordserver.handler.AuthenticationHandler.channelRead0(AuthenticationHandler.scala:14)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:327)\n",
      "\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:299)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/WWGEYX2KNJRORBC2XW2IVZ422YM62EZK/appcache/application_1680149074397_0013/container_1680149074397_0013_01_000001/pyspark.zip/pyspark/sql/session.py\", line 1034, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self)\n",
      "  File \"/mnt/yarn/usercache/WWGEYX2KNJRORBC2XW2IVZ422YM62EZK/appcache/application_1680149074397_0013/container_1680149074397_0013_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/WWGEYX2KNJRORBC2XW2IVZ422YM62EZK/appcache/application_1680149074397_0013/container_1680149074397_0013_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 196, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: com.amazonaws.emr.recordserver.remote.AuthorizationException: Permission Denied: User WWGEYX2KNJRORBC2XW2IVZ422YM62EZK does not have INSERT permission on sample/ticket_purchase_hist\n",
      "\tat com.amazonaws.emr.recordserver.authz.AuthorizerImpl.authorize(Authorizer.scala:392)\n",
      "\tat com.amazonaws.emr.recordserver.handler.AuthorizationHandler.$anonfun$channelRead0$1(AuthorizationHandler.scala:33)\n",
      "\tat com.amazonaws.emr.recordserver.metrics.Metrics$.withMetering(Metrics.scala:49)\n",
      "\tat com.amazonaws.emr.recordserver.handler.AuthorizationHandler.channelRead0(AuthorizationHandler.scala:33)\n",
      "\tat com.amazonaws.emr.recordserver.handler.AuthorizationHandler.channelRead0(AuthorizationHandler.scala:21)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat com.amazonaws.emr.recordserver.handler.ValidationHandler.channelRead0(ValidationHandler.scala:28)\n",
      "\tat com.amazonaws.emr.recordserver.handler.ValidationHandler.channelRead0(ValidationHandler.scala:17)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat com.amazonaws.emr.recordserver.handler.AuthenticationHandler.channelRead0(AuthenticationHandler.scala:66)\n",
      "\tat com.amazonaws.emr.recordserver.handler.AuthenticationHandler.channelRead0(AuthenticationHandler.scala:14)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:327)\n",
      "\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:299)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_df.createOrReplaceTempView(\"tmp_table\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {targetDBName}.{targetTableName}\n",
    "    SELECT *\n",
    "    FROM tmp_table\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37133a74",
   "metadata": {},
   "source": [
    "**The above cell's output expects to see error, like  `Permission Denied: User XXXX does not have INSERT permission on sample/ticket_purchase_hist`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44eafa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
