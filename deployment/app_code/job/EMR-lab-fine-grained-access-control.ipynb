{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceed863",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install sagemaker-studio-analytics-extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5bf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bf409",
   "metadata": {},
   "source": [
    "## Act as data engineer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f4dba",
   "metadata": {},
   "source": [
    "### 1. Configure Apache Hudi for Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8450685",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\n",
    "    \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar\",\n",
    "    \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "    \"spark.sql.extensions\":\"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df115874",
   "metadata": {},
   "source": [
    "### 2. Connect to EMR Cluster via Livy as the ENGINEER_ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8651c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "source ~/.bash_profile\n",
    "EMR_CLUSTER_ID=$(aws emr list-clusters --active  --query 'Clusters[?contains(Name,`emr-roadshow-runtime-role-lf`)].Id' --output text)\n",
    "\n",
    "echo $ENGINEER_ROLE\n",
    "echo $EMR_CLUSTER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sm_analytics emr connect \\\n",
    "--cluster-id j-Q87F82QR17AT \\\n",
    "--auth-type Basic_Access \\\n",
    "--emr-execution-role-arn arn:aws:iam::260906135353:role/lf-data-access-engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%spark\n",
    "\n",
    "# hdfs dfs -mkdir -p /apps/hudi/lib\n",
    "# hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd086d4",
   "metadata": {},
   "source": [
    "### 3. Show session information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6bbf56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457eb53",
   "metadata": {},
   "source": [
    "### 4. Obtain S3 Data Lake bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e04f48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo $DATALAKE_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c57126",
   "metadata": {},
   "source": [
    "### 5. Getting started Apache Hudi and Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06417216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import concat, col, lit, to_timestamp, dense_rank, desc, count, rand, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "rawS3TablePath = \"s3://${DATALAKE_BUCKET}/raw/ticket_purchase_hist/\"\n",
    "hudiTablePath = \"s3://${DATALAKE_BUCKET}/hudi/\"\n",
    "cdcTablePath = \"s3://${DATALAKE_BUCKET}/cdc/ticket_purchase_hist/\"\n",
    "\n",
    "targetDBName = 'hudi_sample'\n",
    "targetTableName = 'hudi_ticket_purchase_hist'\n",
    "targetPath = os.path.join(hudiTablePath, targetDBName, targetTableName)\n",
    "\n",
    "primaryKey = \"sporting_event_ticket_id\"\n",
    "\n",
    "hudiStorageType = 'CoW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea59fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS ' + targetDBName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hudi Table\n",
    "commonConfig = {\n",
    "    'className' : 'org.apache.hudi', \n",
    "    'hoodie.datasource.hive_sync.use_jdbc':'false', \n",
    "    'hoodie.datasource.write.precombine.field': 'transaction_date_time', \n",
    "    'hoodie.datasource.write.recordkey.field': primaryKey, \n",
    "    'hoodie.table.name': targetTableName, \n",
    "    'hoodie.consistency.check.enabled': 'true', \n",
    "    'hoodie.datasource.hive_sync.database': targetDBName, \n",
    "    'hoodie.datasource.hive_sync.table': targetTableName, \n",
    "    'hoodie.datasource.hive_sync.enable': 'true',\n",
    "    'hoodie.datasource.hive_sync.mode': \"hms\"\n",
    "}\n",
    "\n",
    "unpartitionDataConfig = {\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', \n",
    "    'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator'\n",
    "}\n",
    "\n",
    "initLoadConfig = {\n",
    "    'hoodie.bulkinsert.shuffle.parallelism': 3, \n",
    "    'hoodie.datasource.write.operation': 'bulk_insert'\n",
    "}\n",
    "\n",
    "incrementalConfig = {\n",
    "    'hoodie.upsert.shuffle.parallelism': 20, \n",
    "    'hoodie.datasource.write.operation': 'upsert', \n",
    "    'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', \n",
    "    'hoodie.cleaner.commits.retained': 10\n",
    "}\n",
    "\n",
    "dropColumnList = ['db','table_name','Op']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d328bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input raw dataframe\n",
    "inputDf = spark.read.option(\"header\", True).csv(rawS3TablePath)\n",
    "inputDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a141a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDf.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3982505",
   "metadata": {},
   "source": [
    "## Login as an analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c3229",
   "metadata": {},
   "source": [
    "### 1. Connect to EMR Cluster via Livy as ANALYST_ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64129bdf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "echo $ANALYST_ROLE\n",
    "echo $EMR_CLUSTER_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sm_analytics emr connect \\\n",
    "--cluster-id <CLUSTER_ID> \\\n",
    "--auth-type Basic_Access \\\n",
    "--emr-execution-role-arn <ANALYST_ROLE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42651457",
   "metadata": {},
   "source": [
    "### 2. test the column-level permission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use hudi_sample\")\n",
    "# spark.sql(\"desc formatted cpa_hudi_ticket_purchase_hist\").show(100, False)\n",
    "spark.sql(\"SELECT * FROM hudi_sample.cpa_hudi_ticket_purchase_hist limit 10\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
