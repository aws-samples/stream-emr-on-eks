{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5bf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install sagemaker-studio-analytics-extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0213fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a8482",
   "metadata": {},
   "source": [
    "## 1. Get EMR Cluster ID, IAM roles and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20935159",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "source ~/.bash_profile\n",
    "CLUSTER=$(aws emr list-clusters --active  --query 'Clusters[?contains(Name,`emr-roadshow-runtime-role-lf`)].Id' --output text)\n",
    "\n",
    "echo \"EMR ClusterId: $CLUSTER\"\n",
    "echo \"ENGINEER_ROLE: $ENGINEER_ROLE\"\n",
    "echo \"ANALYST_ROLE: $ANALYST_ROLE\"\n",
    "echo \"Data Lake S3 bucket name: $DATALAKE_BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e53e3e",
   "metadata": {},
   "source": [
    "## 2. Upload sample data to datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb412e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "aws s3api get-object --bucket aws-dataengineering-day.workshop.aws --key data/dms_sample/ticket_purchase_hist/LOAD00000001.csv --range bytes=1-10000 ticket_purchase_hist.csv\n",
    "aws s3 cp ticket_purchase_hist.csv s3://$DATALAKE_BUCKET/raw/\n",
    "    \n",
    "# expecting access deny error    \n",
    "aws s3 ls s3://$DATALAKE_BUCKET/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bf409",
   "metadata": {},
   "source": [
    "## 3. Submit job to EMR as a data engineer \n",
    "### can create databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f378187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sm_analytics emr connect \\\n",
    "--cluster-id j-ZZWQ7QVK9O3B \\\n",
    "--auth-type Basic_Access \\\n",
    "--emr-execution-role-arn arn:aws:iam::312026938062:role/lf-data-access-engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8450685",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\n",
    "    \"spark.jars\":\"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar\",\n",
    "    \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "    \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "    \"spark.sql.extensions\":\"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06417216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import concat, col, lit, to_timestamp, dense_rank, desc, count, rand, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "rawS3TablePath = \"s3://lf-datalake-312026938062-us-west-2/raw/\"\n",
    "hudiTablePath = \"s3://lf-datalake-312026938062-us-west-2/hudi/\"\n",
    "# cdcTablePath = \"s3://lf-datalake-312026938062-us-west-2/cdc/\"\n",
    "\n",
    "targetDBName = 'hudi_sample'\n",
    "targetTableName = 'hudi_ticket_purchase_hist'\n",
    "targetPath = os.path.join(hudiTablePath, targetDBName, targetTableName)\n",
    "\n",
    "primaryKey = \"sporting_event_ticket_id\"\n",
    "\n",
    "hudiStorageType = 'CoW'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea59fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS ' + targetDBName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hudi Table\n",
    "commonConfig = {\n",
    "    'className' : 'org.apache.hudi', \n",
    "    'hoodie.datasource.hive_sync.use_jdbc':'false', \n",
    "    'hoodie.datasource.write.precombine.field': 'transaction_date_time', \n",
    "    'hoodie.datasource.write.recordkey.field': primaryKey, \n",
    "    'hoodie.table.name': targetTableName, \n",
    "    'hoodie.consistency.check.enabled': 'true', \n",
    "    'hoodie.datasource.hive_sync.database': targetDBName, \n",
    "    'hoodie.datasource.hive_sync.table': targetTableName, \n",
    "    'hoodie.datasource.hive_sync.enable': 'true',\n",
    "    'hoodie.datasource.hive_sync.mode': \"hms\"\n",
    "}\n",
    "\n",
    "unpartitionDataConfig = {\n",
    "    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', \n",
    "    'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator'\n",
    "}\n",
    "\n",
    "initLoadConfig = {\n",
    "    'hoodie.bulkinsert.shuffle.parallelism': 3, \n",
    "    'hoodie.datasource.write.operation': 'bulk_insert'\n",
    "}\n",
    "\n",
    "incrementalConfig = {\n",
    "    'hoodie.upsert.shuffle.parallelism': 20, \n",
    "    'hoodie.datasource.write.operation': 'upsert', \n",
    "    'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', \n",
    "    'hoodie.cleaner.commits.retained': 10\n",
    "}\n",
    "\n",
    "dropColumnList = ['db','table_name','Op']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d328bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input raw dataframe\n",
    "inputDf = spark.read.option(\"header\", True).csv(rawS3TablePath)\n",
    "inputDf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a141a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDf.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3982505",
   "metadata": {},
   "source": [
    "## 4. Login as an analyst \n",
    "### test the column-level permission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sm_analytics emr connect \\\n",
    "--cluster-id YOUR_EMR_CLUSTERID \\\n",
    "--auth-type Basic_Access \\\n",
    "--emr-execution-role-arn arn:aws:iam::633458367150:role/lf-data-access-analyst\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use hudi_sample\")\n",
    "# spark.sql(\"desc formatted cpa_hudi_ticket_purchase_hist\").show(100, False)\n",
    "spark.sql(\"SELECT * FROM hudi_sample.hudi_ticket_purchase_hist limit 10\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
