{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81deb244",
   "metadata": {},
   "source": [
    "# 0. Load sagemaker_studio_analytics_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker-studio-analytics-extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5bf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86bf409",
   "metadata": {},
   "source": [
    "# 1. Act as data engineer \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In this section, we will create Spark application with *data engineer* EMR runtime role, which is designed as a Lake Formation database and table creator. Then, we will performce Create, Read, Insert, Drop, a.k.a CRUD, actions on the databases and tables that are governed by Lake Formation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df115874",
   "metadata": {},
   "source": [
    "## 1.1. Connect to EMR cluster via Livy as the ENGINEER_ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a8651c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "source ~/.bash_profile\n",
    "EMR_CLUSTER_ID=$(aws emr list-clusters --active  --query 'Clusters[?contains(Name,`emr-roadshow-runtime-role-lf`)].Id' --output text)\n",
    "echo \"CLUSTER_ID:   $EMR_CLUSTER_ID\"\n",
    "echo \"IAM_ARN:      $ENGINEER_ROLE\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e748c00",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> In the following command, replace <b>&ltCLUSTER_ID&gt</b> with 'cluster ID', and <b>&ltIAM_ARN&gt</b> with 'IAM ARN' using the output from the above cell. \n",
    "\n",
    "    \n",
    "Example:<br>\n",
    "<code>\n",
    "%sm_analytics emr connect \\\n",
    "--cluster-id j-1NEOGU3MXB8YT \\\n",
    "--auth-type Basic_Access \\\n",
    "--emr-execution-role-arn arn:aws:iam::012345678:role/lf-data-access-engineer\n",
    "</code>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sm_analytics emr connect \\\n",
    "--cluster-id <CLUSTER_ID> \\\n",
    "--auth-type Basic_Access \\\n",
    "--emr-execution-role-arn <IAM_ARN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457eb53",
   "metadata": {},
   "source": [
    "## 1.2. Config parameters for S3 data lake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e04f48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "source ~/.bash_profile\n",
    "echo \"DATALAKE_BUCKET:    $DATALAKE_BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8faa910",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Replace <code>\"DATALAKE_BUCKET\"</code> with the output from the above cell.<br><b>For exmaple:</b> DATALAKE_BUCKET=\"lf-datalake-676072755675-us-east-1\"\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALAKE_BUCKET=\"DATALAKE_BUCKET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06417216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import concat, col, lit, to_timestamp, dense_rank, desc, count, rand, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "rawS3TablePath = f\"s3://{DATALAKE_BUCKET}/raw/ticket_purchase_hist/\"\n",
    "outputS3Path = f\"s3://{DATALAKE_BUCKET}/output/\"\n",
    "\n",
    "targetDBName = 'sample'\n",
    "targetTableName = 'ticket_purchase_hist'\n",
    "targetPath = os.path.join(outputS3Path, targetDBName, targetTableName)\n",
    "\n",
    "targetDBName2 = 'sample2'\n",
    "targetTableName2 = 'ticket_purchase_hist2'\n",
    "targetPath2 = os.path.join(outputS3Path, targetDBName2, targetTableName2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f3a02",
   "metadata": {},
   "source": [
    "## 1.3. Create 2 database and 2 tables governed by Lake Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea59fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {targetDBName} LOCATION '{outputS3Path}{targetDBName}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca18112",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {targetDBName}.{targetTableName} (\n",
    "        sporting_event_ticket_id string,\n",
    "        purchased_by_id string,\n",
    "        transaction_date_time string,\n",
    "        transferred_from_id string,\n",
    "        purchase_price string\n",
    "    )\n",
    "    USING PARQUET\n",
    "    OPTIONS(\n",
    "        'path' '{targetPath}'\n",
    "    )\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c11343",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {targetDBName2} LOCATION '{outputS3Path}{targetDBName2}'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {targetDBName2}.{targetTableName2} LIKE {targetDBName}.{targetTableName}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974d19e",
   "metadata": {},
   "source": [
    "## 1.4 Insert data to a table governed by Lake Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2eef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDf = spark.read.option(\"header\", True).csv(rawS3TablePath).limit(10)\n",
    "\n",
    "inputDf.write.insertInto(f\"{targetDBName}.{targetTableName}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc15143",
   "metadata": {},
   "source": [
    "## 1.5. Read data from the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {targetDBName}.{targetTableName}\n",
    "\"\"\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c932d",
   "metadata": {},
   "source": [
    "## 1.6. Drop a database & table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7bb6a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    DROP TABLE {targetDBName2}.{targetTableName2}\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    DROP DATABASE {targetDBName2}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3982505",
   "metadata": {},
   "source": [
    "# 2. Act as data analyst \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Please go back to workshop instruction <b>Lab: EMR on EC2</b>, Section: <b>Use EMR Runtime Roles with Sagemaker notebook</b>, Step: <b>Grant read-only permission to Data Analyst role on Lake Formation</b> to configure the data access before running the following cells.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "In this section, we will connect an existing EMR cluster with a *data analyst* role, which is designed to be a business consumer that has a fine-grained access control by Lake Formation.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c3229",
   "metadata": {},
   "source": [
    "## 2.1. Connect to EMR cluster via Livy as ANALYST_ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64129bdf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "source ~/.bash_profile\n",
    "EMR_CLUSTER_ID=$(aws emr list-clusters --active  --query 'Clusters[?contains(Name,`emr-roadshow-runtime-role-lf`)].Id' --output text)\n",
    "echo \"CLUSTER_ID:   $EMR_CLUSTER_ID\"\n",
    "echo \"IAM_ARN:      $ANALYST_ROLE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sm_analytics emr connect \\\n",
    "--cluster-id <CLUSTER_ID> \\\n",
    "--auth-type Basic_Access \\\n",
    "--emr-execution-role-arn <IAM_ARN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066fd3d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "source ~/.bash_profile\n",
    "echo \"DATALAKE_BUCKET:    $DATALAKE_BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841519df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Replace <code>\"DATALAKE_BUCKET\"</code> with the output from the above cell. \n",
    "    <br><b>For exmaple:</b> DATALAKE_BUCKET=\"lf-datalake-676072755675-us-east-1\"\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c16847",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALAKE_BUCKET=\"DATALAKE_BUCKET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e05ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import concat, col, lit, to_timestamp, dense_rank, desc, count, rand, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "outputS3Path = f\"s3://{DATALAKE_BUCKET}/output/\"\n",
    "\n",
    "rawS3TablePath = f\"s3://{DATALAKE_BUCKET}/raw/ticket_purchase_hist/\"\n",
    "\n",
    "targetDBName = 'sample'\n",
    "targetTableName = 'ticket_purchase_hist'\n",
    "targetPath = os.path.join(outputS3Path, targetDBName, targetTableName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69eda6",
   "metadata": {},
   "source": [
    "## 2.2 Test the column-level permission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b2b8c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The expected output is to display only 2 columns that are granted by Lake Formation for the analyst role\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {targetDBName}.{targetTableName}\n",
    "\"\"\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00a7c8",
   "metadata": {},
   "source": [
    "## 2.3 Test insert permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8dcd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_df = spark.read.option(\"header\", True).csv(rawS3TablePath).sample(fraction=0.1).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04dfed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_df.createOrReplaceTempView(\"tmp_table\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {targetDBName}.{targetTableName}\n",
    "    SELECT *\n",
    "    FROM tmp_table\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37133a74",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The expected output from above cell is an error message, like  <code>Permission Denied: User XXXX does not have INSERT permission on sample/ticket_purchase_hist</code>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
